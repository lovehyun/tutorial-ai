단계	분류 모델	난이도	설명
1단계	K-최근접 이웃 (KNN)	⭐	가장 직관적, 개념이 쉽고 계산 기반
2단계	의사결정트리 (Decision Tree)	⭐⭐	규칙 기반, 시각화가 쉬움
3단계	로지스틱 회귀 (Logistic Regression)	⭐⭐	확률적 분류, 선형 모델
4단계	나이브 베이즈 (Naive Bayes)	⭐⭐	확률 기반, 단순하지만 빠름
5단계	서포트 벡터 머신 (SVM)	⭐⭐⭐	경계선 기반, 강력하지만 직관은 어려움
6단계	랜덤 포레스트 (Random Forest)	⭐⭐⭐	앙상블 모델, 고성능
7단계	그래디언트 부스팅 (Gradient Boosting)	⭐⭐⭐⭐	최고성능 앙상블, 복잡하지만 실무에서 많이 사용

✅ 1. K-최근접 이웃 (KNN) – 가장 쉬운 분류
개념
- 새로운 데이터를 예측할 때 가장 가까운 k개의 이웃을 보고 다수결로 라벨을 결정.

핵심 특징
- 단순하고 직관적
- 모델 학습 불필요 → 저장만 하면 바로 예측 가능
- 거리 계산이 핵심

필수 전처리
- 스케일링 필수 (StandardScaler)

✅ 2. 의사결정트리 (Decision Tree)
개념
- 데이터가 yes/no로 갈라지는 트리 구조를 만들고, 최종 잎 노드에서 클래스를 예측.

핵심 특징
- 직관적
- 시각화가 쉬움
- 스케일링 필요 없음

✅ 3. 로지스틱 회귀 (Logistic Regression)
개념
- 선형 방정식 기반으로 확률을 예측하고, 임계값(보통 0.5)으로 분류.

핵심 특징
- 이진/다중 클래스 모두 가능
- 확률 기반 분류
- 스케일링 필수

✅ 4. 나이브 베이즈 (Naive Bayes)
개념
- 베이즈 정리를 기반으로 각 특성의 독립성을 가정하고 확률 계산으로 분류.

핵심 특징
- 단순하지만 빠르고 효율적
- 스케일링 필요 없음

✅ 5. 서포트 벡터 머신 (SVM)
개념
- 데이터의 가장 넓은 간격의 경계선을 찾는 분류기

핵심 특징
- 복잡한 데이터도 잘 분류
- 스케일링 필수
- 시간이 오래 걸릴 수 있음

✅ 6. 랜덤 포레스트 (Random Forest)
개념
- 여러 개의 Decision Tree를 랜덤으로 생성하여 평균/다수결로 예측

핵심 특징
- 강력한 성능
- 스케일링 불필요
- 오버피팅 방어

✅ 7. 그래디언트 부스팅 (Gradient Boosting)
개념
- 여러 개의 약한 모델을 순차적으로 학습하여 강한 모델 생성

핵심 특징
- 실무에서 가장 많이 사용
- XGBoost, LightGBM 등이 대표
- 복잡하지만 최고의 성능
